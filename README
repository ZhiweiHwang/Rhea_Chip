Rhea_chip is a simple pipeline for the analysis of human data generated by NGS approaches. With Rhea_chip you can now have full suite of NGS tools up and running on any high end workstation in an afternoon.

This pipeline will provide a workflow as follows:

From the fastq files generated from the Illumina pipeline, all of them were first processed by filtering adaptor sequences and removing low-quality reads. Cleaned reads were aligned to the GRCh37/hg19 human reference genome (UCSC Genome Browser) with aln/mem and sampe tools of the Burrows-Wheeler Aligner (BWA)(Li & Durbin, 2010). The resulting bam files were sorted by using the SortSam tools which is part of the Picard toolkit (http://sourceforge.net/projects/picard/). The Picard MergeSamFiles tool was performed to merge the sorted bam files which belong to one sample into one bam file. Potential PCR artifacts were marked with the Picard MarkDuplicates tool. Notably, alignment to the human reference particularly in areas around small base pair insertions and deletions (indels) is often imperfect which may misalign. To facilitate the identification of Indels, alignments were refined with the Genome Analysis Toolkit(GATK) RealignerTargetCreator and IndelRealigner tools. When used together, these tools use the full alignment context to determine whether an Indel exists. Quality scores for sequencing reads were recalibrated using BaseRecalibrator and PrintReads from GATK to more closely match the actual probability of mismatching the reference and to correct any variation in quality between machine cycle and sequence context(DePristo et al., 2011). An index file for each BAM file was generated using the SAMTools index function. Variants were called using the HaplotypeCaller from GATK (https://www.broadinstitute.org/gatk/). HaplotypeCaller calls both single nucleotide polymorphisms and indels using de novo assembly of haplotypes in the target region. After variants were called and exported in variant call format (VCF), we performed the GATK tool VariantFiltration to filter variants. SNV and indel VCF files were combined using GATK CombineVariants tool.

1. PRE-PROCESSING
When you receive sequence data from your sequencing provider (whether it is an in-house service or a commercial company), the data is typically in a raw state (one or several FASTQ files) that is not immediately usable for variant discovery analysis. Even if you receive a BAM file (i.e. a file in which the reads have been aligned to a reference genome) you still need to apply some data processing in order to maximize the technical correctness of the data.

This first phase of the Best Practices workflow describes the pre-processing steps that are necessary in order to prepare your data for analysis, starting with FASTQ or uBAM files and ending in an analysis-ready BAM file.

We begin by mapping the sequence reads to the reference genome to produce a file in SAM/BAM format sorted by coordinate. Next, we mark duplicates to mitigate biases introduced by data generation steps such as PCR amplification. When working with RNAseq, we apply an RNA-specific processing step to split reads that span splice junctions, and we assign suitable mapping qualities to all well-mapped reads. Then, for both DNA and RNA, we perform local realignment around indels, because the algorithms that are used in the initial mapping step tend to produce various types of artifacts in the regions around indels. Finally, we recalibrate the base quality scores, because the variant calling algorithms rely heavily on the quality scores assigned to the individual base calls in each sequence read.

2. Map to Reference
The variant discovery workflow depends on having sequence data in the form of reads that are aligned to a reference genome. So the very first step is of course to map your reads to the reference to produce a file in SAM/BAM format.

We recommend using BWA MEM for DNA and STAR for RNAseq, but depending on your data and how it was sequenced, you may need to use a different aligner. Read group information is typically added during this step, but can also be added or modified after mapping using Picard AddOrReplaceReadGroups.

These steps are performed with tools that are not part of GATK, so we don't provide detailed documentation of the methods involved and all the options available. For more details, please see those tools' respective documentation websites (linked below).

3. Mark Duplicates
Once your data has been mapped to the reference genome, you can proceed to mark duplicates. The idea here is that during the sequencing process, the same DNA fragments may be sequenced several times. The resulting duplicate reads are not informative and should not be counted as additional evidence for or against a putative variant. The duplicate marking process (sometimes called **dedupping** in bioinformatics slang) does not remove the reads, but identifies them as duplicates by adding a flag in the read's SAM record. Most GATK tools will then ignore these duplicate reads by default, through the internal application of a read filter. The duplicate read filter can be disabled for certain purposes (e.g. for differential allele expression analysis).

Duplicate marking should NOT be applied to amplicon sequencing data or other data types where reads start and stop at the same positions by design.

This step is performed with a Picard tool, which is not part of GATK, so we don't provide detailed documentation of all the options available. For more details, please see the Picard documentation website (linked below).

4. Realign Indels
The algorithms that are used in the initial mapping step tend to produce various types of artifacts. For example, reads that align on the edges of indels often get mapped with mismatching bases that might look like evidence for SNPs, but are actually mapping artifacts. The realignment process identifies the most consistent placement of the reads relative to the indel in order to clean up these artifacts. It occurs in two steps: first the program identifies intervals that need to be realigned, then in the second step it determines the optimal consensus sequence and performs the actual realignment of reads.

This step used to be very important when the the variant callers were position-based (such as UnifiedGenotyper) but now that we have assembly-based variant callers (such as HaplotypeCaller) it is less important. We still perform indel realignment because we think it may improve the accuracy of the base recalibration model in the next step, but this step may be made obsolete in the near future.

5. Recalibrate Bases
Variant calling algorithms rely heavily on the quality scores assigned to the individual base calls in each sequence read. These scores are per-base estimates of error emitted by the sequencing machines. Unfortunately the scores produced by the machines are subject to various sources of systematic technical error, leading to over- or under-estimated base quality scores in the data. Base quality score recalibration (BQSR) is a process in which we apply machine learning to model these errors empirically and adjust the quality scores accordingly. This allows us to get more accurate base qualities, which in turn improves the accuracy of our variant calls.

The base recalibration process involves two key steps: first the program builds a model of covariation based on the data and a set of known variants (which you can bootstrap if there is none available for your organism), then it adjusts the base quality scores in the data based on the model.

There is an optional but highly recommended step that involves building a second model and generating before/after plots to visualize the effects of the recalibration process. This is useful for quality control purposes.

Note that this base recalibration process should not be confused with variant recalibration, which is a sophisticated filtering technique applied on the variant callset produced in a later step.

6. VARIANT DISCOVERY
Once you have pre-processed your data according to our recommendations, you are ready to undertake the variant discovery process, i.e. identify the sites where your data displays variation relative to the reference genome, and calculate genotypes for each sample at that site.

Unfortunately some of the variation you might observe is caused by mapping and sequencing artifacts, so the greatest challenge here is to balance the need for sensitivity (to minimize false negatives, i.e. failing to identify real variants) vs. specificity (to minimize false positives, i.e. failing to reject artifacts). We have found that it is very difficult to reconcile these objectives in a single step, so instead we decompose the variant discovery process into two separate steps: variant calling and variant filtering. The first step is designed to maximize sensitivity, while the filtering step aims to deliver a level of specificity that can be customized for each project.

For DNA, the variant calling step is further subdivided into two separate steps (per-sample calling followed by joint genotyping across samples) in order to enable scalable and incremental processing of cohorts comprising many individual samples (see DNA workflow for details). This advanced workflow has not yet been validated for use with RNAseq data.

The best way to filter the resulting variant callset is to use variant quality score recalibration (VQSR), which uses machine learning to identify annotation profiles of variants that are likely to be real. The drawback of this sophisticated method is that it requires a large callset (minimum 30 exomes, more than 1 whole genome if possible) and highly curated sets of known variants. This makes it difficult to apply to small experiments, RNAseq experiments, and non-model organisms; for those it is typically necessary to develop hard filtering parameters manually.

7. Call Variants
In the past, variant callers specialized in either SNPs or Indels, or (like the GATK's own UnifiedGenotyper) could call both but had to do so them using separate models of variation. The HaplotypeCaller is capable of calling SNPs and indels simultaneously via local de-novo assembly of haplotypes in an active region. In other words, whenever the program encounters a region showing signs of variation, it discards the existing mapping information and completely reassembles the reads in that region. This allows the HaplotypeCaller to be more accurate when calling regions that are traditionally difficult to call, for example when they contain different types of variants close to each other. It also makes the HaplotypeCaller much better at calling indels than position-based callers like UnifiedGenotyper.

In the so-called GVCF mode used for scalable variant calling in DNA sequence data, HaplotypeCaller runs per-sample to generate an intermediate genomic gVCF (gVCF), which can then be used for joint genotyping of multiple samples in a very efficient way, which enables rapid incremental processing of samples as they roll off the sequencer, as well as scaling to very large cohort sizes (e.g. the 92K exomes of ExAC).

In addition, HaplotypeCaller is able to handle non-diploid organisms as well as pooled experiment data. Note however that the algorithms used to calculate variant likelihoods is not well suited to extreme allele frequencies (relative to ploidy) so its use is not recommended for somatic (cancer) variant discovery. For that purpose, use MuTect instead.

Finally, HaplotypeCaller is also able to correctly handle the splice junctions that make RNAseq a challenge for most variant callers.

8. Merge (optional)
This is an optional step that applies only to the DNA workflow. It consists of merging gVCF files hierarchically in order to reduce the number of files that must be handled simultaneously in the next step. This is only necessary if you are working with more than a few hundred samples. Note that this is NOT equivalent to the joint genotyping step; variants in the resulting merged gVCF cannot be considered to have been called jointly.

The number of files to merge together depends on your infrastructure; we have found that merging in batches of 10 to 200 maximum works well. Merging more files at a time reduces the number of merges that need to be done but increases memory requirements and decreases runtime of individual merge operations.

9. Joint Genotype
At this step, which applies only to the DNA workflow, we gather all the per-sample GVCFs (or combined GVCFs if we are working with large numbers of samples) and pass them all together to the joint genotyping tool, GenotypeGVCFs. This produces a set of joint-called SNP and indel calls ready for filtering. This cohort-wide analysis empowers sensitive detection of variants even at difficult sites, and produces a squared-off matrix of genotypes that provides information about all sites of interest in all samples considered, which is important for many downstream analyses.

This step runs very fast and can be rerun at any point when samples are added to the cohort, thereby solving the N+1 problem.

10. Filter Variants
The GATK's variant calling tools are designed to be very lenient in order to achieve a high degree of sensitivity. This is good because it minimizes the chance of missing real variants, but it does mean that we need to filter the raw callset they produce in order to reduce the amount of false positives, which can be quite large.

The best way to filter the raw variant callset is to use variant quality score recalibration (VQSR), which uses machine learning to identify annotation profiles of variants that are likely to be real, and assigns a VQSLOD score to each variant that is much more reliable than the QUAL score calculated by the caller. In the first step of this two-step process, the program builds a model based on training variants, then applies that model to the data to assign a well-calibrated probability to each variant call. We can then use this variant quality score in the second step to filter the raw call set, thus producing a subset of calls with our desired level of quality, fine-tuned to balance specificity and sensitivity.

The downside of how variant recalibration works is that the algorithm requires high-quality sets of known variants to use as training and truth resources, which for many organisms are not yet available. It also requires quite a lot of data in order to learn the profiles of good vs. bad variants, so it can be difficult or even impossible to use on small datasets that involve only one or a few samples, on targeted sequencing data, on RNAseq, and on non-model organisms. If for any of these reasons you find that you cannot perform variant recalibration on your data (after having tried the workarounds that we recommend, where applicable), you will need to use hard-filtering instead. This consists of setting flat thresholds for specific annotations and applying them to all variants equally. See the methods articles and FAQs for more details on how to do this.

11. CALLSET REFINEMENT
Once you have generated and filtered your callset according to our recommendations, you have several options for evaluating and refining the variant and genotype calls further, before moving on with your study.

In this last section, we perform some refinement steps on the genotype calls based on population frequencies and pedigree information if available, add functional annotations related to predicted biological effect, and do some quality evaluation by comparing the callset to known resources. None of these steps are absolutely required, and the workflow may need to be adapted to each project's requirements.

Please note that we are currently reevaluating our recommendations for parts of this analysis phase in order to provide better guidance. We expect to publish improved documentation on this topic in September 2015.

12. Refine GTs
The core GATK Best Practices workflow has historically focused on variant discovery --that is, the existence of genomic variants in one or more samples in a cohorts-- and consistently delivers high quality results when applied appropriately. However, we know that the quality of the individual genotype calls coming out of the variant callers can vary widely based on the quality of the BAM data for each sample. The goal of the Genotype Refinement workflow is to use additional data to improve the accuracy of genotype calls and to filter genotype calls that are not reliable enough for downstream analysis. In this sense it serves as an optional extension of the variant calling workflow, intended for researchers whose work requires high-quality identification of individual genotypes.

13. Annotate Variants
